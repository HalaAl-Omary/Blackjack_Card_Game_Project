{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalaAl-Omary/Blackjack_Card_Game_Project/blob/main/Model_2_Section_2_part_4_Training_Your_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Week 7 part 2: Training Your Neural Network\n",
        "\n",
        "> **üéØ Today‚Äôs Goal**: Learn how to **train** your neural network using **loss functions**, **optimizers**, and **gradient descent** in PyTorch.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ñ∂Ô∏è Today‚Äôs Video\n",
        "\n",
        "üìΩÔ∏è Watch this essential tutorial on training neural networks in PyTorch:\n",
        "\n",
        "> üí° *Tip: Pay special attention to the 4-step training loop: Forward ‚Üí Loss ‚Üí Backward ‚Üí Step. This is the heartbeat of deep learning.*\n"
      ],
      "metadata": {
        "id": "fLldnf2yhQst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Building a Neural Network with PyTorch in 15 Minutes | Coding Challenge\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# Create the HTML for embedding\n",
        "html_code = f\"\"\"\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V_xro1bcAuA?si=ppBpi6ciGbKhGHuh\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
        "\n",
        "\"\"\"\n",
        "# Display the video\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "_lvmr6fHiIrS",
        "outputId": "d124f984-5e5f-4819-dab9-ad73ebff838d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "\n",
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V_xro1bcAuA?si=ppBpi6ciGbKhGHuh\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
              "\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìñ Today‚Äôs Theory: The Training Loop\n",
        "\n",
        "Training a neural network isn‚Äôt magic ‚Äî it‚Äôs a **structured, repeating process**:\n",
        "\n",
        "1. **üîÅ Forward Pass**  \n",
        "   ‚Üí Input data flows through the network ‚Üí produces predictions.\n",
        "\n",
        "2. **üìâ Compute Loss**  \n",
        "   ‚Üí Compare predictions to true labels using a **loss function** (e.g., CrossEntropyLoss).\n",
        "\n",
        "3. **‚¨ÖÔ∏è Backward Pass (Backpropagation)**  \n",
        "   ‚Üí Calculate how much each weight contributed to the error ‚Üí `loss.backward()`.\n",
        "\n",
        "4. **‚öôÔ∏è Optimizer Step**  \n",
        "   ‚Üí Update weights to reduce error ‚Üí `optimizer.step()` + `optimizer.zero_grad()`.\n",
        "\n",
        "üîÅ Repeat for every batch, for multiple **epochs** (full passes over the dataset).\n",
        "\n",
        "> üß† **Think of it like tuning a radio**:  \n",
        "> You hear static (loss) ‚Üí twist the dial (gradients) ‚Üí clearer signal (lower loss) ‚Üí repeat until perfect.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Day 4 Exercise: Train Your MNIST Classifier\n",
        "\n",
        "> ‚úçÔ∏è **Your Task**: Complete the **training loop** for your `Net` model from Day 3.  \n",
        "> You‚Äôll train for **2 epochs** ‚Äî just enough to see the loss decrease and learning begin!\n",
        "\n",
        "---\n",
        "\n",
        "<div style=\"background-color:#fff3cd; padding:15px; border-left: 5px solid #ffc107; border-radius: 5px; margin: 20px 0;\">\n",
        "üìå <strong>Instructions</strong>:<br>\n",
        "‚Ä¢ Copy the <strong>Day 4 Code</strong> (provided in the next cell) into a new code cell.<br>\n",
        "‚Ä¢ Look for <code># TODO</code> comments ‚Äî these are your tasks.<br>\n",
        "‚Ä¢ Once done, run the <strong>Self-Assessment cell</strong> to verify your training loop works.\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Day 4 Self-Assessment\n",
        "\n",
        "> üß™ After completing the training loop, run the self-assessment code in a new cell.  \n",
        "> It will check if your model‚Äôs weights actually updated ‚Äî proving your training loop works!\n",
        "\n",
        "---\n",
        "\n",
        "> üåü **You‚Äôre now officially training neural networks ‚Äî not just building them. Tomorrow: we measure how well they learned.**"
      ],
      "metadata": {
        "id": "3ncvNaiRLJ7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Day 4 Exercise Code ==========\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- 1. Data (Same as Day 3) ---\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# --- 2. Model (Use your Net from Day 3) ---\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # Input layer ‚Üí hidden layer\n",
        "        self.relu = nn.ReLU()             # Activation\n",
        "        self.fc2 = nn.Linear(128, 10)     # Hidden layer ‚Üí output (10 digits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten 28x28 image to 784-dim vector\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# --- 3. Loss Function & Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss()           # Standard for multi-class classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "# --- 4. üöÄ TRAINING LOOP ‚Äî YOUR TURN! ---\n",
        "print(\"üöÄ Starting training for 2 epochs...\")\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "        # ‚úèÔ∏è TODO 1: Zero out the gradients from the previous step\n",
        "        # (Hint: Use optimizer.zero_grad())\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ‚úèÔ∏è TODO 2: Forward pass ‚Äî get model predictions\n",
        "        # (Hint: outputs = model(inputs))\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # ‚úèÔ∏è TODO 3: Compute the loss between predictions and true labels\n",
        "        # (Hint: loss = criterion(outputs, labels))\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # ‚úèÔ∏è TODO 4: Backward pass ‚Äî compute gradients\n",
        "        # (Hint: loss.backward())\n",
        "        loss.backward()\n",
        "\n",
        "        # ‚úèÔ∏è TODO 5: Update the weights using the optimizer\n",
        "        # (Hint: optimizer.step())\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track and print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 batches\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"‚úÖ Training complete! You‚Äôve trained your first neural network!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwmySt2eLsmz",
        "outputId": "67e10925-b038-4bf7-98fc-679baf19cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 38.5MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 1.03MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 9.48MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 14.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting training for 2 epochs...\n",
            "Epoch [1/2], Step [100/938], Loss: 1.9072\n",
            "Epoch [1/2], Step [200/938], Loss: 1.1684\n",
            "Epoch [1/2], Step [300/938], Loss: 0.8064\n",
            "Epoch [1/2], Step [400/938], Loss: 0.6447\n",
            "Epoch [1/2], Step [500/938], Loss: 0.5537\n",
            "Epoch [1/2], Step [600/938], Loss: 0.5102\n",
            "Epoch [1/2], Step [700/938], Loss: 0.4785\n",
            "Epoch [1/2], Step [800/938], Loss: 0.4396\n",
            "Epoch [1/2], Step [900/938], Loss: 0.4258\n",
            "Epoch [2/2], Step [100/938], Loss: 0.3956\n",
            "Epoch [2/2], Step [200/938], Loss: 0.3895\n",
            "Epoch [2/2], Step [300/938], Loss: 0.3762\n",
            "Epoch [2/2], Step [400/938], Loss: 0.3753\n",
            "Epoch [2/2], Step [500/938], Loss: 0.3824\n",
            "Epoch [2/2], Step [600/938], Loss: 0.3720\n",
            "Epoch [2/2], Step [700/938], Loss: 0.3510\n",
            "Epoch [2/2], Step [800/938], Loss: 0.3435\n",
            "Epoch [2/2], Step [900/938], Loss: 0.3299\n",
            "‚úÖ Training complete! You‚Äôve trained your first neural network!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Day 4 Self-Assessment ==========\n",
        "\n",
        "#@title Run this to check your training loop\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def check_day4_training_task():\n",
        "    feedback = []\n",
        "    score = 0\n",
        "    total = 1\n",
        "\n",
        "    try:\n",
        "        # Save initial weights\n",
        "        initial_weight = model.fc1.weight.clone().detach()\n",
        "\n",
        "        # Run one mini-batch manually to test training mechanics\n",
        "        inputs, labels = next(iter(trainloader))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Check if weights changed ‚Üí proof that training works\n",
        "        if not torch.equal(initial_weight, model.fc1.weight):\n",
        "            score += 1\n",
        "            feedback.append(\"‚úÖ **Training Loop Verified**: Model weights updated ‚Äî your loop is working correctly!\")\n",
        "        else:\n",
        "            feedback.append(\"‚ùå **Training Loop Issue**: Weights did NOT change. Did you forget `.backward()` or `.step()`?\")\n",
        "\n",
        "    except Exception as e:\n",
        "        feedback.append(f\"‚ùå **Error in Self-Assessment**: {e}\")\n",
        "\n",
        "    # Final message\n",
        "    final_message = \"**üéØ Day 4 Self-Assessment Results**\\n\\n\" + \"\\n\".join(feedback)\n",
        "    final_message += f\"\\n\\nüìä **Score: {score}/{total}**\"\n",
        "\n",
        "    if score == 1:\n",
        "        final_message += \"\\n\\nüéâ Amazing! You‚Äôve successfully implemented the training loop. Ready for Day 5 ‚Äî evaluating your model‚Äôs performance!\"\n",
        "    else:\n",
        "        final_message += \"\\n\\nüîß Double-check your 5 TODOs. Did you call all steps in the correct order?\"\n",
        "\n",
        "    display(Markdown(final_message))\n",
        "\n",
        "# Run the check\n",
        "check_day4_training_task()"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "1yu0jRR6LwAf",
        "outputId": "5b6560e6-d8e5-4911-be2b-91a003d0601c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**üéØ Day 4 Self-Assessment Results**\n\n‚úÖ **Training Loop Verified**: Model weights updated ‚Äî your loop is working correctly!\n\nüìä **Score: 1/1**\n\nüéâ Amazing! You‚Äôve successfully implemented the training loop. Ready for Day 5 ‚Äî evaluating your model‚Äôs performance!"
          },
          "metadata": {}
        }
      ]
    }
  ]
}